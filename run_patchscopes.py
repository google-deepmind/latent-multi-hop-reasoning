# Copyright 2025 DeepMind Technologies Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Run Patchscopes analysis to investigate latent entity representations in multi-hop reasoning as described in the paper 'Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?'."""
import argparse
import os

from src import data_utils
from src import evaluation_utils
from src import model_utils
from src import patchscopes_utils
from src import tokenization_utils
import transformers

AutoModelForCausalLM = transformers.AutoModelForCausalLM
AutoTokenizer = transformers.AutoTokenizer


def get_parser():
  """Returns an argument parser for the run_patchscopes script."""
  parser = argparse.ArgumentParser(
      description=(
          "Run Patchscopes analysis to investigate latent entity"
          " representations in multi-hop reasoning as described in the paper"
          " 'Do Large Language Models Perform Latent Multi-Hop Reasoning"
          " without Exploiting Shortcuts?'"
      )
  )

  parser.add_argument(
      "--model_name_or_path",
      type=str,
      default="mistralai/Mistral-7B-v0.3",
      help=(
          "Path to pretrained model or model identifier from"
          " huggingface.co/models."
      ),
  )
  parser.add_argument(
      "--revision",
      type=str,
      default=None,
      help="Specific model revision to use from HuggingFace.",
  )
  parser.add_argument(
      "--input_csv_path",
      type=str,
      default="datasets/SOCRATES.csv",
      help="Path to the input CSV file containing multi-hop queries.",
  )
  parser.add_argument(
      "--hf_token",
      type=str,
      default=os.environ.get("HF_TOKEN_PATH", None),
      help=(
          "HuggingFace token for accessing models. Defaults to environment"
          " variable HF_TOKEN_PATH."
      ),
  )
  parser.add_argument(
      "--batch_size",
      type=int,
      default=128,
      help="Batch size for processing queries through the model.",
  )
  parser.add_argument(
      "--num_return_sequences",
      type=int,
      default=3,
      help=(
          "Number of generations to sample for each patched hidden state"
          " (default=3 following paper)."
      ),
  )
  parser.add_argument(
      "--source_layer_idxs",
      default="",
      type=str,
      help=(
          "Comma-separated list of source layer indices to extract hidden"
          " states from. If empty, uses all layers."
      ),
  )
  parser.add_argument(
      "--target_layer_idxs",
      default="",
      type=str,
      help=(
          "Comma-separated list of target layer indices to patch hidden states"
          " into. If empty, uses all layers."
      ),
  )
  parser.add_argument(
      "--greedy",
      action="store_true",
      help="Use greedy decoding instead of sampling with temperature=1.0.",
  )
  parser.add_argument(
      "--run_patchscopes_evaluation",
      action="store_true",
      help="Evaluate the completions generated by Patchscopes.",
  )
  parser.add_argument(
      "--run_evaluation",
      action="store_true",
      help=(
          "Run completions of the fact composition types and shortcut-free"
          " evaluation on the generated completions."
      ),
  )
  parser.add_argument(
      "--output_dir",
      type=str,
      default="results/run_patchscopes",
      help="Directory to save experimental results and generations.",
  )
  return parser


def main(args):
  model_utils.set_random_seed(42)
  print(vars(args))

  model_name_or_path = args.model_name_or_path
  safe_model_name_or_path = "/".join(
      model_name_or_path.strip("/").split("/")[-2:]
  )
  safe_model_name_or_path = safe_model_name_or_path.replace("/", "--")

  print(f"Loading {model_name_or_path}")
  kwargs = {}
  if args.revision:
    kwargs = {"revision": args.revision}
    print(f"Revision: {args.revision}")
    safe_model_name_or_path += f".{args.revision}"

  model = AutoModelForCausalLM.from_pretrained(
      model_name_or_path,
      device_map="auto",
      attn_implementation="sdpa",
      token=args.hf_token,
      **kwargs,
  ).eval()
  tokenizer = AutoTokenizer.from_pretrained(
      model_name_or_path,
      token=args.hf_token,
  )

  if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
  tokenizer.padding_side = "left"

  input_csv_path = args.input_csv_path
  safe_model_name_or_path = model_name_or_path.replace("/", "--")
  input_csv_name = os.path.basename(input_csv_path).replace(".csv", "")

  experiment_dir = args.output_dir
  os.makedirs(experiment_dir, exist_ok=True)
  print(f"Saving results to {experiment_dir}")

  output_csv_path = os.path.join(
      experiment_dir, f"{input_csv_name}.{safe_model_name_or_path}.csv"
  )

  print(f"Reading {input_csv_path}")
  df = data_utils.read_dataframe(input_csv_path)

  if model_utils.is_instruction_tuned(model, model_name_or_path):
    fact_type = "r2(r1(e1)).blank"
    print("Instruction-tuned model")
  else:
    fact_type = "r2(r1(e1))"
    print("Pretrained model")

  if f"{fact_type}.subject_cut.prompt" not in df:
    df.loc[:, f"{fact_type}.subject_cut.prompt"] = df.apply(
        lambda row: tokenization_utils.get_subject_prompt(
            row[f"{fact_type}.prompt"], row["mu.value"]
        ),
        axis=1,
    )

  source_layer_idxs = (
      list(map(int, args.source_layer_idxs.split(",")))
      if args.source_layer_idxs
      else None
  )
  target_layer_idxs = (
      list(map(int, args.target_layer_idxs.split(",")))
      if args.target_layer_idxs
      else None
  )
  df = run(
      model,
      tokenizer,
      df,
      fact_type,
      source_layer_idxs,
      target_layer_idxs,
      args.batch_size,
      args.greedy,
      args.num_return_sequences,
      args.run_evaluation,
      args.run_patchscopes_evaluation,
  )

  df.to_csv(output_csv_path, index=False)
  print(f"Saved the dataframe to {output_csv_path}")


def run(
    model,
    tokenizer,
    df,
    fact_type,
    source_layer_idxs,
    target_layer_idxs,
    batch_size,
    greedy,
    num_return_sequences,
    run_evaluation,
    run_patchscopes_evaluation,
):
  """Runs Patchscopes analysis on the given dataframe.

  Args:
    model: The model to use for patching.
    tokenizer: The tokenizer to use for tokenizing the prompts.
    df: The dataframe to run Patchscopes analysis on.
    fact_type: The fact type to use for patching.
    source_layer_idxs: The source layer indices to extract hidden states from.
    target_layer_idxs: The target layer indices to patch hidden states into.
    batch_size: The batch size to use for processing queries through the model.
    greedy: Whether to use greedy decoding instead of sampling with
      temperature=1.0.
    num_return_sequences: The number of generations to sample for each patched
      hidden state.
    run_evaluation: Whether to evaluate the completions generated by
      Patchscopes.
    run_patchscopes_evaluation: Whether to run shortcut-free evaluation on the
      generated completions.

  Returns:
    The dataframe with the patched completions added.
  """
  if source_layer_idxs is None:
    source_layer_idxs = [i for i in range(model.config.num_hidden_layers)]
  if target_layer_idxs is None:
    target_layer_idxs = [i for i in range(model.config.num_hidden_layers)]

  df.loc[:, "patchscopes.prompt"] = (
      "StarCraft: StarCraft is a science fiction real-time strategy game, "
      + "Leonardo DiCaprio: Leonardo DiCaprio is an American actor, "
      + "Samsung: Samsung is a South Korean multinational corporation, "
      + "x"
  )

  batch_get_completions_from_patching = data_utils.batchify(
      patchscopes_utils.get_completions_from_patching,
      batch_size=batch_size,
      flush_step=1,
  )
  results = batch_get_completions_from_patching(
      {
          "prompts": df[f"{fact_type}.prompt"].tolist(),
          "subject_prompts": df[f"{fact_type}.subject_cut.prompt"].tolist(),
          "target_prompts": df["patchscopes.prompt"].tolist(),
      },
      model=model,
      tokenizer=tokenizer,
      num_return_sequences=num_return_sequences,
      do_sample=not greedy,
      source_layer_idxs=source_layer_idxs,
      target_layer_idxs=target_layer_idxs,
  )

  for (
      token_position,
      k,
      source_index,
      target_index,
  ), result in results.items():
    df.loc[
        :,
        f"{fact_type}.patched.{token_position}-{k}-{source_index}-{target_index}.completion",
    ] = result

  if run_patchscopes_evaluation:
    print("Running Patchscopes evaluation")
    evaluation_utils.run_patchscopes_evaluation(
        df,
        f"{fact_type}.patched",
        source_layer_idxs,
        target_layer_idxs,
        num_return_sequences,
    )

  if run_evaluation:
    print("Running shortcut-free evaluation")
    evaluation_utils.run_shortcut_free_completion(
        df,
        model,
        tokenizer,
        model.name_or_path,
        batch_size=batch_size * 2,
        backend="hf",
        force_completion=True,
    )
    evaluation_utils.run_shortcut_free_evaluation(df)
    df = evaluation_utils.get_df_with_shortcut_free_metrics(df)

  return df


if __name__ == "__main__":
  main(get_parser().parse_args())
